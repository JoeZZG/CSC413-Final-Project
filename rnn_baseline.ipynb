{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Environment Setup**\n",
        "\n",
        "First, install all necessary libraries. Also use spacy for tokenization and other libraries for data handling and visualization."
      ],
      "metadata": {
        "id": "X92UByT0iI14"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iY3xlch5iG65"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117\n",
        "!pip install spacy\n",
        "!pip install tqdm seaborn\n",
        "!python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**\n",
        "\n",
        "Import all the required libraries for data processing, model building, training, and visualization."
      ],
      "metadata": {
        "id": "_4mnwNUkiMz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm  # Progress bar\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import spacy\n",
        "from collections import Counter\n"
      ],
      "metadata": {
        "id": "_j8f4zqEiOkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Seeds and Device**\n",
        "\n",
        "Setting seeds ensures reproducibility of results. Check if a GPU is available for faster computation."
      ],
      "metadata": {
        "id": "jE-osEHoiQx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print PyTorch and CUDA information\n",
        "print(\"PyTorch Version:\", torch.__version__)\n",
        "print(\"CUDA Available:\", torch.cuda.is_available())\n",
        "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
        "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "id": "phn1Cr-SiRcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare Dataset**\n",
        "\n",
        "Use the pre-extracted dataset. They data is organized into CSV files with one sentence per line."
      ],
      "metadata": {
        "id": "qkwlica2iTXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to read a CSV file with one sentence per line\n",
        "def read_csv_file(filepath, label, max_samples=None):\n",
        "    data = []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if max_samples and i >= max_samples:\n",
        "                break\n",
        "            text = line.strip()\n",
        "            if text:\n",
        "                data.append((label, text))\n",
        "    return data\n",
        "\n",
        "\n",
        "DATA_DIR = \"/content\"\n",
        "\n",
        "\n",
        "# Specify the maximum number of samples per file (e.g., 5000)\n",
        "MAX_SAMPLES = 5000\n",
        "\n",
        "# Read the CSV files\n",
        "train_pos = read_csv_file(os.path.join(DATA_DIR, 'train_pos.csv'), label=1, max_samples=MAX_SAMPLES)\n",
        "train_neg = read_csv_file(os.path.join(DATA_DIR, 'train_neg.csv'), label=0, max_samples=MAX_SAMPLES)\n",
        "test_pos = read_csv_file(os.path.join(DATA_DIR, 'test_pos.csv'), label=1, max_samples=MAX_SAMPLES)\n",
        "test_neg = read_csv_file(os.path.join(DATA_DIR, 'test_neg.csv'), label=0, max_samples=MAX_SAMPLES)\n",
        "\n",
        "# Optionally read unsupervised training data if available\n",
        "# train_unsup = read_csv_file(os.path.join(DATA_DIR, 'train_unsup.csv'), label=None, max_samples=MAX_SAMPLES)\n",
        "# train_data = train_pos + train_neg + train_unsup\n",
        "# Otherwise, proceed without unsupervised data\n",
        "train_data = train_pos + train_neg\n",
        "test_data = test_pos + test_neg\n",
        "\n",
        "print(f\"Total training samples: {len(train_data)}\")\n",
        "print(f\"Total test samples: {len(test_data)}\")"
      ],
      "metadata": {
        "id": "KPUUsK9jiUPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization and Vocabulary Building**\n",
        "\n",
        "Tokenization splits sentences into words (tokens). Use spacy for efficient tokenization. After tokenizing, willbuild a vocabulary mapping each unique token to an integer index."
      ],
      "metadata": {
        "id": "91z_N8FyiVVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy English model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"spaCy model loaded successfully.\")\n",
        "except OSError:\n",
        "    print(\"spaCy model not found. Downloading 'en_core_web_sm'...\")\n",
        "    from spacy.cli import download\n",
        "    download(\"en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"spaCy model loaded successfully after downloading.\")\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize(text, nlp):\n",
        "    return [tok.text.lower() for tok in nlp(text)]\n",
        "\n",
        "# Example tokenization\n",
        "sample_text = \"This is a fantastic movie! I loved every moment of it.\"\n",
        "tokens = tokenize(sample_text, nlp)\n",
        "print(f\"Tokens: {tokens}\")\n",
        "\n",
        "# Build Vocabulary\n",
        "texts = [text for label, text in train_data]\n",
        "\n",
        "# Initialize Counter\n",
        "counter = Counter()\n",
        "\n",
        "# Tokenize all texts and update the counter\n",
        "print(\"Building vocabulary...\")\n",
        "for doc in tqdm(nlp.pipe(texts, batch_size=1000, disable=[\"parser\", \"ner\"]), total=len(texts), desc=\"Tokenizing\"):\n",
        "    tokens = [tok.text.lower() for tok in doc]\n",
        "    counter.update(tokens)\n",
        "\n",
        "# Keep top 25,000 words\n",
        "most_common = counter.most_common(25000)\n",
        "vocab_words = [word for word, freq in most_common]\n",
        "\n",
        "# Add special tokens\n",
        "special_tokens = ['<pad>', '<unk>']\n",
        "stoi = {w: i + len(special_tokens) for i, w in enumerate(vocab_words)}\n",
        "stoi['<pad>'] = 0\n",
        "stoi['<unk>'] = 1\n",
        "itos = {i: w for w, i in stoi.items()}\n",
        "\n",
        "print(f\"Vocabulary size: {len(stoi)}\")\n"
      ],
      "metadata": {
        "id": "G166YYziiWAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create PyTorch Dataset and DataLoader**\n",
        "\n",
        "Create a custom Dataset class to handle our data and a DataLoader for batching. Padding ensures that all sequences in a batch have the same length."
      ],
      "metadata": {
        "id": "FR_hDzNgiXa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom Dataset\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, data, stoi, nlp, max_length=None):\n",
        "        self.data = data\n",
        "        self.stoi = stoi\n",
        "        self.nlp = nlp\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label, text = self.data[idx]\n",
        "        tokens = tokenize(text, self.nlp)\n",
        "        numericalized = [self.stoi.get(t, self.stoi['<unk>']) for t in tokens]\n",
        "        if self.max_length:\n",
        "            numericalized = numericalized[:self.max_length]\n",
        "        return torch.tensor(numericalized, dtype=torch.long), torch.tensor(label, dtype=torch.float)\n",
        "\n",
        "# Define a Collate Function for DataLoader\n",
        "class CollateFn:\n",
        "    def __init__(self, pad_idx):\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        texts, labels = zip(*batch)\n",
        "        lengths = torch.tensor([len(x) for x in texts], dtype=torch.long)\n",
        "        padded_texts = pad_sequence(texts, batch_first=True, padding_value=self.pad_idx)\n",
        "        return padded_texts, lengths, torch.tensor(labels, dtype=torch.float)\n",
        "\n",
        "# Split training data into train and validation sets\n",
        "random.shuffle(train_data)\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(train_data) * split_ratio)\n",
        "valid_data = train_data[split_index:]\n",
        "train_data = train_data[:split_index]\n",
        "\n",
        "print(f\"Train samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(valid_data)}\")\n",
        "print(f\"Test samples: {len(test_data)}\")\n",
        "\n",
        "# Create datasets\n",
        "MAX_LENGTH = 256\n",
        "train_dataset = IMDBDataset(train_data, stoi, nlp, max_length=MAX_LENGTH)\n",
        "valid_dataset = IMDBDataset(valid_data, stoi, nlp, max_length=MAX_LENGTH)\n",
        "test_dataset = IMDBDataset(test_data, stoi, nlp, max_length=MAX_LENGTH)\n",
        "\n",
        "# Create DataLoaders\n",
        "PAD_IDX = stoi['<pad>']\n",
        "collate_fn_instance = CollateFn(pad_idx=PAD_IDX)\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "NUM_WORKERS = 4",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn_instance,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "valid_loader = DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn_instance,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn_instance,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"DataLoaders created.\")\n"
      ],
      "metadata": {
        "id": "gAPqNQe9iYAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the RNN Model**\n",
        "\n",
        "Define an RNN model with LSTM layers. The model includes an embedding layer, LSTM, dropout, and a fully connected layer for binary classification."
      ],
      "metadata": {
        "id": "uVj24MaciZMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Sentiment RNN model\n",
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=n_layers,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout if n_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "    def forward(self, text, lengths):\n",
        "        embedded = self.embedding(text)\n",
        "        # Pack the sequences\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
        "            embedded, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "        if self.bidirectional:\n",
        "            hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1, :, :])\n",
        "        output = self.fc(hidden)\n",
        "        return output.squeeze(1)\n"
      ],
      "metadata": {
        "id": "YwqU9lehiZ4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and Evaluation**\n",
        "\n",
        "Define the training and evaluation functions. Use Binary Cross-Entropy loss with logits and the Adam optimizer. Also track accuracy during training."
      ],
      "metadata": {
        "id": "q3Fih089iaoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Model, Optimizer, Loss\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "\n",
        "model = SentimentRNN(\n",
        "    vocab_size=len(stoi),\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    output_dim=OUTPUT_DIM,\n",
        "    n_layers=N_LAYERS,\n",
        "    bidirectional=BIDIRECTIONAL,\n",
        "    dropout=DROPOUT,\n",
        "    pad_idx=PAD_IDX\n",
        ")\n",
        "\n",
        "# Initialize embeddings for special tokens\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[stoi['<unk>']] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "# Define accuracy metric\n",
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch.\n",
        "    \"\"\"\n",
        "    # Apply sigmoid to get probabilities\n",
        "    probs = torch.sigmoid(preds)\n",
        "    # Round to get binary predictions\n",
        "    rounded = torch.round(probs)\n",
        "    correct = (rounded == y).float()\n",
        "    return correct.sum() / len(correct)\n",
        "\n",
        "# Training function\n",
        "def train_fn(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    for texts, lengths, labels in tqdm(loader, desc=\"Training\"):\n",
        "        # Move data to device\n",
        "        texts, lengths, labels = texts.to(device), lengths.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(texts, lengths)\n",
        "        loss = criterion(predictions, labels)\n",
        "        acc = binary_accuracy(predictions, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(loader), epoch_acc / len(loader)\n",
        "\n",
        "# Evaluation function\n",
        "def eval_fn(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    with torch.no_grad():\n",
        "        for texts, lengths, labels in tqdm(loader, desc='Evaluating'):\n",
        "            texts, lengths, labels = texts.to(device), lengths.to(device), labels.to(device)\n",
        "            predictions = model(texts, lengths)\n",
        "            loss = criterion(predictions, labels)\n",
        "            acc = binary_accuracy(predictions, labels)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    return epoch_loss / len(loader), epoch_acc / len(loader)\n"
      ],
      "metadata": {
        "id": "XQK5JWdoibHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the Model**\n",
        "\n",
        "Train the model for a specified number of epochs, saving the best model based on validation loss."
      ],
      "metadata": {
        "id": "6fnsJ84ric24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Model\n",
        "N_EPOCHS = 5\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "train_losses, valid_losses = [], []\n",
        "train_accuracies, valid_accuracies = [], []\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    print(f\"Epoch: {epoch + 1}/{N_EPOCHS}\")\n",
        "    train_loss, train_acc = train_fn(model, train_loader, optimizer, criterion, device)\n",
        "    valid_loss, valid_acc = eval_fn(model, valid_loader, criterion, device)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    valid_accuracies.append(valid_acc)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'best-model.pt')\n",
        "        print(f\"  Best model saved (Validation Loss: {valid_loss:.4f})\")\n",
        "\n",
        "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc * 100:.2f}%\")\n",
        "    print(f\"  Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc * 100:.2f}%\\n\")\n"
      ],
      "metadata": {
        "id": "6OYjDbU2idwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization**\n",
        "\n",
        "Visualize the training and validation loss and accuracy over epochs using matplotlib and seaborn."
      ],
      "metadata": {
        "id": "rge9QQnSieuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization\n",
        "sns.set_style(\"darkgrid\")\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, N_EPOCHS + 1), train_losses, label='Train Loss')\n",
        "plt.plot(range(1, N_EPOCHS + 1), valid_losses, label='Valid Loss')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss over Epochs\")\n",
        "plt.legend()\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, N_EPOCHS + 1), train_accuracies, label='Train Acc')\n",
        "plt.plot(range(1, N_EPOCHS + 1), valid_accuracies, label='Valid Acc')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy over Epochs\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OnWdVZnUifSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate on Test Data\n",
        "Load the best model and evaluate its performance on the test dataset."
      ],
      "metadata": {
        "id": "oW5wuDPoigjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model\n",
        "model.load_state_dict(torch.load('best-model.pt'))\n",
        "\n",
        "# Evaluate on test data\n",
        "test_loss, test_acc = eval_fn(model, test_loader, criterion, device)\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "udFbZrgTihCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion Matrix**\n",
        "\n",
        "Generate a confusion matrix to visualize the performance of the model on the test set."
      ],
      "metadata": {
        "id": "NhmGI63xihoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get predictions and labels\n",
        "def get_predictions(model, loader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for texts, lengths, labels in tqdm(loader, desc='Predicting'):\n",
        "            texts, lengths = texts.to(device), lengths.to(device)\n",
        "            predictions = torch.sigmoid(model(texts, lengths))\n",
        "            preds = torch.round(predictions)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "    return all_preds, all_labels\n",
        "\n",
        "# Get predictions and labels\n",
        "test_preds, test_labels = get_predictions(model, test_loader, device)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(test_labels, test_preds)\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=['Negative', 'Positive'])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix on Test Set')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6M3V-onBiiGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predict on Custom Sentences**\n",
        "\n",
        "Use the trained model to predict the sentiment of custom movie reviews."
      ],
      "metadata": {
        "id": "l0Ur6OlKijDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict sentiment of a single sentence\n",
        "def predict_sentiment(model, text, stoi, nlp, device, max_length=256):\n",
        "    model.eval()\n",
        "    tokens = tokenize(text, nlp)\n",
        "    numericalized = [stoi.get(t, stoi['<unk>']) for t in tokens]\n",
        "    if len(numericalized) > max_length:\n",
        "        numericalized = numericalized[:max_length]\n",
        "    lengths = torch.tensor([len(numericalized)], dtype=torch.long).to(device)\n",
        "    tensor = torch.tensor(numericalized, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction = torch.sigmoid(model(tensor, lengths))\n",
        "    return prediction.item()\n",
        "\n",
        "# Sample reviews\n",
        "sample_reviews = [\n",
        "    \"This movie was absolutely fantastic, I loved it!\",\n",
        "    \"I hated this movie, it was so bad and boring.\",\n",
        "    \"It was okay, not great but not terrible either.\",\n",
        "]\n",
        "\n",
        "# Predict and display results\n",
        "for review in sample_reviews:\n",
        "    score = predict_sentiment(model, review, stoi, nlp, device)\n",
        "    sentiment = \"Positive\" if score >= 0.5 else \"Negative\"\n",
        "    print(f\"Review: {review}\\nPredicted: {sentiment} (Score: {score:.4f})\\n\")\n"
      ],
      "metadata": {
        "id": "H6CUBIi9ijlp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
