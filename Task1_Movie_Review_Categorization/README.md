# Movie Review Categorization with RNN and Transformer

This repository contains implementations of two baseline models for sentiment analysis on the IMDB movie reviews dataset:

1. **RNN Baseline**: A Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) layers.
2. **Transformer Baseline**: A custom Transformer model leveraging self-attention mechanisms.

These baselines establish benchmarks for future experiments with more advanced models.

## Table of Contents

- [Movie Review Categorization with RNN and Transformer](#movie-review-categorization-with-rnn-and-transformer)
  - [Table of Contents](#table-of-contents)
  - [Introduction](#introduction)
  - [Dataset](#dataset)
    - [Dataset Repository](#dataset-repository)
    - [Dataset Preparation](#dataset-preparation)
  - [Baseline Models](#baseline-models)
    - [RNN Baseline](#rnn-baseline)
    - [Transformer Baseline](#transformer-baseline)
  - [Training Process](#training-process)
  - [Evaluation](#evaluation)
  - [Data Augmentation Tests](#data-augmentation-tests)
  - [Additional Resources](#additional-resources)
  - [License](#license)

## Introduction

Sentiment analysis is a key task in Natural Language Processing (NLP) that involves classifying the sentiment expressed in text. This project uses the IMDB movie reviews dataset to develop two baseline models. The goal is to classify movie reviews as either positive or negative.

## Dataset

### Dataset Repository

The dataset used in this project is sourced from the [IMDB Movie Reviews Dataset](https://ai.stanford.edu/~amaas/data/sentiment/).

### Dataset Preparation

**Steps Taken**:
1. The dataset, originally in text format, was converted into CSV files for easier handling.
2. Files are organized as follows:
   - `train_neg.csv`: Negative reviews for training.
   - `train_pos.csv`: Positive reviews for training.
   - `test_neg.csv`: Negative reviews for testing.
   - `test_pos.csv`: Positive reviews for testing.
3. Data was tokenized and numericalized into fixed-length sequences.

## Baseline Models

### RNN Baseline

The RNN baseline model uses Long Short-Term Memory (LSTM) layers. It consists of:
- **Embedding Layer**: Transforms input tokens into dense vector representations.
- **LSTM Layers**: Captures temporal dependencies in the sequence.
- **Fully Connected Layer**: Maps the LSTM outputs to a binary classification task.

### Transformer Baseline

The Transformer baseline leverages self-attention mechanisms for text representation. The key components include:
- **Embedding Layer**: Maps input tokens to dense vector embeddings.
- **Positional Encoding**: Adds sequence position information to embeddings.
- **Transformer Encoder Layers**: Captures contextual information through multi-head attention and feedforward networks.
- **Classification Head**: Maps the Transformer outputs to a binary classification task.

**Note**: This Transformer implementation is custom-built using PyTorch's `nn.TransformerEncoder` and does **not** utilize pre-trained Transformer architectures like BERT.

## Training Process

Both models were trained using the same pipeline:
1. Preprocessed data into tokenized and numericalized sequences.
2. Split data into training, validation, and test sets.
3. Employed data augmentation techniques to enhance model robustness.
4. Used PyTorch DataLoaders for efficient batching.
6. Saved the best-performing model based on validation loss.

## Evaluation

Model performance was evaluated using accuracy and confusion matrices. Additionally, visualizations of training and validation loss/BLEU scores were plotted over epochs. BLEU scores were computed to assess the quality of translations generated by the Transformer model.

## Data Augmentation Tests

To enhance model robustness and performance, various data augmentation strategies were employed. These tests involve modifying the proportion of positive samples and applying synonym-based augmentations to the dataset.


**Descriptions of Augmentation Tests**:

- **Proportion-Based Augmentations**:
  - `rnn_20%pos.ipynb` to `rnn_80%pos.ipynb`: Experiments varying the percentage of positive samples in the RNN training data.
  - `transformer_20%pos.ipynb` to `transformer_80%pos.ipynb`: Similar experiments for the Transformer baseline.

- **Training Data Size Adjustments**:
  - `rnn_50%_less_training_data.ipynb` & `rnn_50%_more_training_data.ipynb`: Testing the impact of reducing or increasing the amount of training data for the RNN model.
  - `transformer_50%_less_training_data.ipynb` & `transformer_50%_more_training_data.ipynb`: Similar adjustments for the Transformer model.

- **Synonym-Based Augmentations**:
  - `rnn_synonym_augmentation_20%.ipynb` & `rnn_synonym_augmentation_50%.ipynb`: Augmenting RNN training data by replacing words with synonyms in 20% and 50% of sentences.
  - `transformer_synonym_augmentation_20%.ipynb` & `transformer_synonym_augmentation_50%.ipynb`: Similar synonym replacements for the Transformer baseline.

- **Pre-trained Embedding Augmentations**:
  - `rnn_pre_trained_augmentation.ipynb` & `transformer_pre_trained_augmentation.ipynb`: Incorporating pre-trained GloVe embeddings to enhance model performance.

- **Transformer Specific Augmentations**:
  - `transformer_1head.ipynb` to `transformer_10head.ipynb`: Testing the effect of varying the number of attention heads in the Transformer model.

Each notebook is organized to reflect the structure and changes applied during the augmentation tests, facilitating easy navigation and reproducibility.


## Additional Resources

- **IMDB Movie Reviews Dataset**: The original dataset used in this project for sentiment analysis. [IMDB Dataset](https://ai.stanford.edu/~amaas/data/sentiment/)

- **Sentiment Analysis using RNN-LSTM**: This repository demonstrates sentiment analysis using Recurrent Neural Networks (RNN) with Long Short-Term Memory (LSTM) units, employing the Google News Word2Vec model for word embeddings. [GitHub Repository](https://github.com/saadarshad102/Sentiment-Analysis-RNN-LSTM)
- **GloVe Embeddings**: [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)
   **Description**: Pre-trained word vectors developed by Stanford University, widely used for embedding layers in NLP models.

## License

This project is licensed under the MIT License. See the LICENSE file for details.